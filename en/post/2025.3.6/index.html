<!DOCTYPE html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: December 4, 2025 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Hugo Blox Builder 5.9.7" />
  

  
  












  
  










  







  
  

  
  
  

  
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.e4b9d3e8ce28da563d74b4089f677743.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.63b8f95c9bb6734ca6b02cdff7183fab.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  



























  
  
  






  <meta name="author" content="沈滢" />





  

<meta name="description" content="A total of two papers from the Artificial Intelligence Laboratory (AI Lab) of the Department of Electronics, Southern University of Science and Technology (SUSTech) have been accepted in the acceptance results published by AAAI-2025, the top international conference on Artificial Intelligence, and both of them have been accepted as Oral Presentation." />



  <link rel="alternate" hreflang="zh" href="https://nkdailab.github.io/post/2025.3.6/" />

<link rel="alternate" hreflang="en-us" href="https://nkdailab.github.io/en/post/2025.3.6/" />
<link rel="canonical" href="https://nkdailab.github.io/en/post/2025.3.6/" />



  <link rel="manifest" href="/en/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu_d78793efeb3744c5.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu_ce0f57a661b5b57d.png" />

<meta name="theme-color" content="#1565c0" />










  






<meta property="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://nkdailab.github.io/en/post/2025.3.6/featured.png" />



  

<meta property="og:type" content="article" />
<meta property="og:site_name" content="NKD AI Lab" />
<meta property="og:url" content="https://nkdailab.github.io/en/post/2025.3.6/" />
<meta property="og:title" content="He Zhihai&#39;s group in the Department of Electronics at SUSTech is pleased to present two AAAI-2025 Oral Papers | NKD AI Lab" />
<meta property="og:description" content="A total of two papers from the Artificial Intelligence Laboratory (AI Lab) of the Department of Electronics, Southern University of Science and Technology (SUSTech) have been accepted in the acceptance results published by AAAI-2025, the top international conference on Artificial Intelligence, and both of them have been accepted as Oral Presentation." /><meta property="og:image" content="https://nkdailab.github.io/en/post/2025.3.6/featured.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2025-02-26T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2025-02-26T00:00:00&#43;00:00">
  






    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://nkdailab.github.io/en/post/2025.3.6/"
  },
  "headline": "He Zhihai's group in the Department of Electronics at SUSTech is pleased to present two AAAI-2025 Oral Papers",
  
  "image": [
    "https://nkdailab.github.io/en/post/2025.3.6/featured.png"
  ],
  
  "datePublished": "2025-02-26T00:00:00Z",
  "dateModified": "2025-02-26T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "沈滢"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "NKD AI Lab",
    "logo": {
      "@type": "ImageObject",
      "url": "https://nkdailab.github.io/media/icon_hu_cad48d5bae1d0c97.png"
    }
  },
  "description": "A total of two papers from the Artificial Intelligence Laboratory (AI Lab) of the Department of Electronics, Southern University of Science and Technology (SUSTech) have been accepted in the acceptance results published by AAAI-2025, the top international conference on Artificial Intelligence, and both of them have been accepted as Oral Presentation."
}
</script>

  

  




  
  
  

  
  

  


  
  <title>He Zhihai&#39;s group in the Department of Electronics at SUSTech is pleased to present two AAAI-2025 Oral Papers | NKD AI Lab</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="235108c165bae6c48f0b4c9e042a3472" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.4fef3e534144e9903491f0cc6527eccd.js"></script>

  




  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/en/">NKD AI Lab</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/en/">NKD AI Lab</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/en/tour"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/en/event"><span>Research</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/en/people"><span>People</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/en/publication"><span>Publications</span></a>
          </li>

          
          

          

          
          
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/en/post"><span>News</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        
          
        

        
        
        

        
        
        

        
        
        <li class="nav-item dropdown i18n-dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown"
             aria-haspopup="true" aria-label="Languages">
            <i class="fas fa-globe mr-1" aria-hidden="true"></i><span class="d-none d-lg-inline">English</span></a>
          <div class="dropdown-menu">
            <div class="dropdown-item dropdown-item-active">
              <span>English</span>
            </div>
            
            <a class="dropdown-item" href="https://nkdailab.github.io/post/2025.3.6/">
              <span>中文 (简体)</span>
            </a>
            
          </div>
        </li>
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  













  

  
  
  
<div class="article-container pt-3">
  <h1>He Zhihai&#39;s group in the Department of Electronics at SUSTech is pleased to present two AAAI-2025 Oral Papers</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Feb 26, 2025
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    5 min read
  </span>
  

  
  
  
  

  
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>AAAI, short for The Association for Advancement of Artificial Intelligence, is one of the most important international conferences in the field of Artificial Intelligence, hosted by the International Association for Artificial Intelligence, and recommended by the Chinese Computer Federation (CCF) as a Class A conference. AAAI AAAI 2025 received 12,957 submissions and 3032 papers were accepted, with an acceptance rate of 23.4%, of which the Oral Presentation acceptance rate was 4.6%.AAAI 2025 was held on 25 February - 4 March 2025 in Philadelphia, Pennsylvania, USA.</p>
<p><strong><u>《Cross-Modal Few-Shot Learning with Second-Order Neural Ordinary Differential Equations》</u></strong></br></p>
<p><strong><u>Authors</u></strong></br>
Yi Zhang (Harbin Institute of Technology, Southern University of Science and Technology), Chun-Wun Cheng (University of Cambridge), Junyi He (Southern University of Science and Technology), Zhihai He (Southern University of Science and Technology), Carola-Bibiane Schönlieb (University of Cambridge), Yuyan Chen (Fudan University), Angelica I Aviles-Rivero (Tsinghua University)</p>
<p><strong><u>Brief introduction</u></strong></br>
Yi Zhang, a PhD student in the class of 2021 at the Department of Electronic and Electrical Engineering, Southern University of Science and Technology, proposes an innovative method SONO in the field of cross-modal small-sample learning, which significantly improves the model&rsquo;s generalisation ability in few-sample scenarios through second-order God-frequent differential equations, and effectively solves the overfitting problem due to the scarcity of data in the traditional method.
With the rapid development of multimodal AI technology, how to make the model achieve cross-modal (e.g., image-text) efficient learning under a very small number of labelled samples has become a key challenge. Existing methods generally face bottlenecks such as high risk of overfitting, high consumption of computational resources, and insufficient cross-modal alignment capability. The research team innovatively introduces the second-order God&rsquo;s frequent differential equation into the cross-modal learning framework, enhances the model expression ability through continuous dynamic feature optimisation, and combines the ‘text-as-image’ data enhancement strategy to effectively expand the training data by using the graphic-text correlation feature of CLIP model. Experiments show that the classification accuracy of this method on 11 benchmark datasets, such as ImageNet, is significantly better than that of the existing optimal method for small samples, and shows potential application in data-scarce scenarios, such as medical images.
<img src="fig1.png" width="2048"></p>
<p>This paper is the result of research conducted by Yi Zhang, a PhD student from the Department of Electronics, SUSTech, class of 2021, during his visit to the University of Cambridge, UK, under the supervision of Professor Zhihai He Chair of SUSTech and Assistant Professor Angelica I. Aviles-Rivero, Tsinghua University Qiu Chengtong Mathematical Sciences Centre, during her tenure as a Senior Associate Researcher at the University of Cambridge, and in collaboration with the research institutes of SUSTech, the Cambridge University Department of Applied Department of Mathematics and Theoretical Physics, University of Cambridge, Tsinghua University&rsquo;s Chuchengtong Centre for Mathematical Sciences, Shanghai Key Laboratory of Data Science and other research institutions. Yi Zhang, a joint PhD candidate of SUSTech and Harbin Institute of Technology, is the first author of this paper, while Junhuan Zheng, a PhD student of Cambridge University, Junyi He, an undergraduate student of SUSTech in the class of 2021, Prof. Carola-Bibiane Schönlieb of Cambridge University, and Yuyan Chen, a PhD student of the Shanghai Key Laboratory of Data Science of Fudan University are the authors of this paper, as well as Zhi-Hai He, a chair professor of the Department of Electronics of SUSTech, Angelica I. Aviles-Rivero, Assistant Professor at the Qiu Chengtong Centre for Mathematical Sciences, Tsinghua University, are the corresponding authors of this paper. NUST doctoral student Yi Zhang and undergraduate student Jun He are also from Zhihai He&rsquo;s group.</p>
<p>Paper link: <a href=https://arxiv.org/abs/2412.15813><a href="https://arxiv.org/abs/2412.15813" target="_blank" rel="noopener">https://arxiv.org/abs/2412.15813</a></a></br>
Funding information: This work was supported by the National Natural Science Foundation of China under Key Project No. 62331014.</p>
<p><strong><u>《Conditional Latent Coding with Learnable Synthesized Reference for Deep Image Compression》</u></strong></br></p>
<p><strong><u>Authors</u></strong></br>
Siqi Wu (University of Missouri), Yinda Chen (University of Science and Technology of China), Dong Liu (University of Science and Technology of China), Zhihai He (Southern University of Science and Technology)</p>
<p><strong><u>Brief introduction</u></strong></br>
Siqi Wu, a PhD student in the Department of Electrical and Computer Engineering, University of Missouri, USA, class of 2022, has proposed a conditional latent coding method based on learnable synthetic references to improve the efficiency of deep image compression in collaboration with the University of Science and Technology of China (USTC) during his visit to Southern University of Science and Technology (SUSTech) as a visiting researcher.</p>
<p>With the rapid development of digital technology and the dramatic increase in the amount of image data, efficient image compression techniques are crucial for storing, transmitting and processing image data. Current deep learning methods have made significant progress in image compression, but still face the challenge of effectively exploiting image source correlation while maintaining high reconstruction quality. The article proposes a conditional latent coding method to conditionally encode the input image by dynamically generating a reference representation from an external dictionary. The input image will be matched with the features in the dictionary to exploit the correlation between the input image and the reference dictionary in order to achieve efficient compression and high quality reconstruction of the input image. This thesis achieves a large enhancement on the publicly available datasets Kodak and CLIC, providing useful insights for further research and potential improvements.
<img src="fig2.png" width="888"></p>
<p>Siqi Wu, a PhD student from the University of Missouri, class of 2022, is currently conducting visiting research in Zhihai He&rsquo;s group as a visiting student at the Southern University of Science and Technology (SUSTech) in the U.S.A. She is the co-first author of this paper together with Yinda Chen, a PhD student from the University of Science and Technology of China, class of 2024. Dong Liu, a professor in the Department of Electrical Engineering and Information Science at the University of Science and Technology of China (USTC), is the author of this paper, and Zhihai He, a chair professor in the Department of Electronics at USTC, is the corresponding author of this paper. The University of Missouri is the first unit of the paper, and Southern University of Science and Technology is the third unit of the paper.</p>
<p>Paper link: <a href=https://arxiv.org/abs/2502.09971v1><a href="https://arxiv.org/abs/2502.09971v1" target="_blank" rel="noopener">https://arxiv.org/abs/2502.09971v1</a></a></br>
Funding information: This work was supported by the National Natural Science Foundation of China under Key Project No. 62331014.</p>

    </div>

    

























  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2025 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.da259f97ed0f170b3547d57d1c732d58.js"></script>




  

  
  

  






  <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>



































<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>










<script src="/en/js/wowchemy.min.e166a4f802c5f09237cc72919197d83b.js"></script>



  <script src="/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js" type="module"></script>




  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.9c0e895144aef5a693008b5c5d450147.js" type="module"></script>


















</body>
</html>
